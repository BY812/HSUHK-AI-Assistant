# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZY8xxtm6BrYFCHs1xItGBgrDEXAJjWbf
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q gradio transformers sentence-transformers torchaudio datasets soundfile pydub

!pip install edge-tts

import json
from sentence_transformers import SentenceTransformer, util
import torch
import gradio as gr
import torchaudio
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import numpy as np
from pydub import AudioSegment
import io
import soundfile as sf
import os
import asyncio
import edge_tts
from pydub.playback import play
import datetime
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# 1. åŠ è½½FAQæ•°æ®
with open('/content/drive/MyDrive/6104/group project/hsu_faq.json') as f:
    faq_data = json.load(f)

# 2. åˆå§‹åŒ–æ¨¡å‹
# è¯­éŸ³è¯†åˆ«æ¨¡å‹(Whisperå¤§å‹ç‰ˆ)
whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")
whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3")

# é—®ç­”åµŒå…¥æ¨¡å‹
embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# æœ¬åœ°LLM(ä½¿ç”¨è¾ƒå°çš„Phi-2æ¨¡å‹)
llm_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2",
                                               torch_dtype=torch.float16,
                                               device_map="auto")
llm_tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")

# 3. é¢„å¤„ç†FAQæ•°æ®
questions = [item['question'] for item in faq_data]
answers = [item['answer'] for item in faq_data]
question_embeddings = embedding_model.encode(questions, convert_to_tensor=True)

# 4. RAGé—®ç­”å‡½æ•°
def rag_qa(user_question, threshold=0.7):
    # ç¼–ç ç”¨æˆ·é—®é¢˜
    user_embedding = embedding_model.encode(user_question, convert_to_tensor=True)

    # è®¡ç®—ç›¸ä¼¼åº¦
    cos_scores = util.cos_sim(user_embedding, question_embeddings)[0]
    max_score_idx = torch.argmax(cos_scores).item()
    max_score = cos_scores[max_score_idx].item()

    if max_score > threshold:
        return answers[max_score_idx]
    else:
        # ä½¿ç”¨LLMç”Ÿæˆå›ç­”
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ’ç”Ÿå¤§å­¦çš„æ™ºèƒ½åŠ©æ‰‹ã€‚æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
        å¦‚æœé—®é¢˜ä¸æ’ç”Ÿå¤§å­¦æ— å…³ï¼Œè¯·ç¤¼è²Œæ‹’ç»å›ç­”ã€‚

        é—®é¢˜: {user_question}
        å›ç­”: """

        inputs = llm_tokenizer(prompt, return_tensors="pt", return_attention_mask=False).to(llm_model.device)
        outputs = llm_model.generate(**inputs, max_new_tokens=100)
        answer = llm_tokenizer.batch_decode(outputs)[0]
        return answer.replace(prompt, "").split("\n")[0]

# 5. æ”¹è¿›çš„è¯­éŸ³å¤„ç†å‡½æ•°
async def text_to_speech(text, output_file="output.mp3"):
    # å®šä¹‰æ–‡æœ¬å’Œè¯­éŸ³å‚æ•°
    voice = "zh-CN-YunjianNeural"  # ä½¿ç”¨ä¸­æ–‡è¯­éŸ³

    # åˆ›å»º Communicate å¯¹è±¡å¹¶ç”Ÿæˆå®Œæ•´éŸ³é¢‘æ–‡ä»¶
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_file)
    return output_file

def transcribe_audio(audio):
    # è¯»å–éŸ³é¢‘å¹¶é‡é‡‡æ ·ä¸º 16kHz
    # Gradio è¿”å›çš„ audio æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« "array" å’Œ "sampling_rate"
    waveform = torch.tensor(audio[1], dtype=torch.float32)
    sample_rate = audio[0]

    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)

    # åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶é¢„å¤„ç†
    audio_input = whisper_processor(waveform, sampling_rate=16000, return_tensors="pt").input_features

    # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬
    with torch.no_grad():
        predicted_ids = whisper_model.generate(audio_input)

    # è§£ç ç”Ÿæˆçš„ token ä¸ºæ–‡æœ¬
    transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

def process_audio(audio_file):
    # å¤„ç†ç©ºè¾“å…¥æƒ…å†µ
    if audio_file is None:
        return "", "è¯·æä¾›è¯­éŸ³è¾“å…¥æˆ–ä½¿ç”¨æ–‡æœ¬è¾“å…¥", None

    try:
        # å¤„ç†GradioéŸ³é¢‘è¾“å…¥
        if isinstance(audio_file, (tuple, list)):
            sample_rate, audio_data = audio_file
            audio_input = (sample_rate, audio_data)
        else:
            # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
            audio_segment = AudioSegment.from_file(audio_file)
            audio_data = np.array(audio_segment.get_array_of_samples())
            if audio_segment.channels > 1:
                audio_data = audio_data.reshape((-1, audio_segment.channels)).mean(axis=1)
            audio_input = (audio_segment.frame_rate, audio_data)

        # è¯­éŸ³è¯†åˆ«
        text = transcribe_audio(audio_input)

        # è·å–å›ç­”
        answer = rag_qa(text)

        # è¯­éŸ³åˆæˆ
        output_audio = "/tmp/output.mp3"
        asyncio.run(text_to_speech(answer, output_audio))

        return text, answer, output_audio

    except Exception as e:
        print(f"å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {str(e)}")
        return "[è¯­éŸ³è¯†åˆ«å¤±è´¥]", f"å¤„ç†è¯­éŸ³è¾“å…¥æ—¶å‡ºé”™: {str(e)}", None

# 6. æ–‡æœ¬å¤„ç†å‡½æ•°
def process_text(text):
    if not text.strip():
        return "", "è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜", None

    answer = rag_qa(text)

    try:
        # è¯­éŸ³åˆæˆ
        output_audio = "/tmp/output.mp3"
        asyncio.run(text_to_speech(answer, output_audio))

        return "", answer, output_audio
    except Exception as e:
        print(f"è¯­éŸ³åˆæˆå¤±è´¥: {str(e)}")
        return "", answer, None

# 7. Gradioç•Œé¢
with gr.Blocks(title="HSUHKæ™ºèƒ½åŠ©æ‰‹") as demo:
    gr.Markdown("# ğŸ¤ HSUHKæ™ºèƒ½è¯­éŸ³åŠ©æ‰‹")
    gr.Markdown("ä¸Šä¼ è¯­éŸ³æé—®æˆ–ä½¿ç”¨éº¦å…‹é£å½•åˆ¶")

    with gr.Tab("è¯­éŸ³è¾“å…¥"):
        audio_input = gr.Audio(sources=["microphone", "upload"], type="numpy", label="è¯­éŸ³è¾“å…¥")
        audio_submit = gr.Button("æäº¤è¯­éŸ³é—®é¢˜")

    with gr.Tab("æ–‡æœ¬è¾“å…¥"):
        text_input = gr.Textbox(label="è¾“å…¥æ–‡å­—é—®é¢˜")
        text_submit = gr.Button("æäº¤æ–‡å­—é—®é¢˜")

    with gr.Column():
        recognized_text = gr.Textbox(label="è¯†åˆ«åˆ°çš„æé—®")
        answer_text = gr.Textbox(label="å›ç­”æ–‡æœ¬")
        answer_audio = gr.Audio(label="è¯­éŸ³å›ç­”", autoplay=True)

    # æ¸…é™¤æŒ‰é’®
    clear_btn = gr.Button("æ¸…é™¤æ‰€æœ‰")

    # äº‹ä»¶å¤„ç†
    audio_submit.click(
        fn=process_audio,
        inputs=audio_input,
        outputs=[recognized_text, answer_text, answer_audio]
    )

    text_submit.click(
        fn=process_text,
        inputs=text_input,
        outputs=[recognized_text, answer_text, answer_audio]
    )

    text_input.submit(
        fn=process_text,
        inputs=text_input,
        outputs=[recognized_text, answer_text, answer_audio]
    )

    clear_btn.click(
        fn=lambda: [None, "", "", None],
        outputs=[audio_input, recognized_text, answer_text, answer_audio]
    )

# å¯åŠ¨åº”ç”¨
demo.launch(debug=True)



