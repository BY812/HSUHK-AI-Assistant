# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZY8xxtm6BrYFCHs1xItGBgrDEXAJjWbf
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q gradio transformers sentence-transformers torchaudio datasets soundfile pydub

!pip install edge-tts

import json
from sentence_transformers import SentenceTransformer, util
import torch
import gradio as gr
import torchaudio
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import numpy as np
from pydub import AudioSegment
import io
import soundfile as sf
import os
import asyncio
import edge_tts
from pydub.playback import play
import datetime
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# 1. 加载FAQ数据
with open('/content/drive/MyDrive/6104/group project/hsu_faq.json') as f:
    faq_data = json.load(f)

# 2. 初始化模型
# 语音识别模型(Whisper大型版)
whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-large-v3")
whisper_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large-v3")

# 问答嵌入模型
embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# 本地LLM(使用较小的Phi-2模型)
llm_model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2",
                                               torch_dtype=torch.float16,
                                               device_map="auto")
llm_tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2")

# 3. 预处理FAQ数据
questions = [item['question'] for item in faq_data]
answers = [item['answer'] for item in faq_data]
question_embeddings = embedding_model.encode(questions, convert_to_tensor=True)

# 4. RAG问答函数
def rag_qa(user_question, threshold=0.7):
    # 编码用户问题
    user_embedding = embedding_model.encode(user_question, convert_to_tensor=True)

    # 计算相似度
    cos_scores = util.cos_sim(user_embedding, question_embeddings)[0]
    max_score_idx = torch.argmax(cos_scores).item()
    max_score = cos_scores[max_score_idx].item()

    if max_score > threshold:
        return answers[max_score_idx]
    else:
        # 使用LLM生成回答
        prompt = f"""你是一个恒生大学的智能助手。根据以下上下文回答问题。
        如果问题与恒生大学无关，请礼貌拒绝回答。

        问题: {user_question}
        回答: """

        inputs = llm_tokenizer(prompt, return_tensors="pt", return_attention_mask=False).to(llm_model.device)
        outputs = llm_model.generate(**inputs, max_new_tokens=100)
        answer = llm_tokenizer.batch_decode(outputs)[0]
        return answer.replace(prompt, "").split("\n")[0]

# 5. 改进的语音处理函数
async def text_to_speech(text, output_file="output.mp3"):
    # 定义文本和语音参数
    voice = "zh-CN-YunjianNeural"  # 使用中文语音

    # 创建 Communicate 对象并生成完整音频文件
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_file)
    return output_file

def transcribe_audio(audio):
    # 读取音频并重采样为 16kHz
    # Gradio 返回的 audio 是一个字典，包含 "array" 和 "sampling_rate"
    waveform = torch.tensor(audio[1], dtype=torch.float32)
    sample_rate = audio[0]

    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)

    # 加载音频文件并预处理
    audio_input = whisper_processor(waveform, sampling_rate=16000, return_tensors="pt").input_features

    # 使用模型生成文本
    with torch.no_grad():
        predicted_ids = whisper_model.generate(audio_input)

    # 解码生成的 token 为文本
    transcription = whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

def process_audio(audio_file):
    # 处理空输入情况
    if audio_file is None:
        return "", "请提供语音输入或使用文本输入", None

    try:
        # 处理Gradio音频输入
        if isinstance(audio_file, (tuple, list)):
            sample_rate, audio_data = audio_file
            audio_input = (sample_rate, audio_data)
        else:
            # 如果是文件路径
            audio_segment = AudioSegment.from_file(audio_file)
            audio_data = np.array(audio_segment.get_array_of_samples())
            if audio_segment.channels > 1:
                audio_data = audio_data.reshape((-1, audio_segment.channels)).mean(axis=1)
            audio_input = (audio_segment.frame_rate, audio_data)

        # 语音识别
        text = transcribe_audio(audio_input)

        # 获取回答
        answer = rag_qa(text)

        # 语音合成
        output_audio = "/tmp/output.mp3"
        asyncio.run(text_to_speech(answer, output_audio))

        return text, answer, output_audio

    except Exception as e:
        print(f"处理音频时出错: {str(e)}")
        return "[语音识别失败]", f"处理语音输入时出错: {str(e)}", None

# 6. 文本处理函数
def process_text(text):
    if not text.strip():
        return "", "请输入有效的问题", None

    answer = rag_qa(text)

    try:
        # 语音合成
        output_audio = "/tmp/output.mp3"
        asyncio.run(text_to_speech(answer, output_audio))

        return "", answer, output_audio
    except Exception as e:
        print(f"语音合成失败: {str(e)}")
        return "", answer, None

# 7. Gradio界面
with gr.Blocks(title="HSUHK智能助手") as demo:
    gr.Markdown("# 🎤 HSUHK智能语音助手")
    gr.Markdown("上传语音提问或使用麦克风录制")

    with gr.Tab("语音输入"):
        audio_input = gr.Audio(sources=["microphone", "upload"], type="numpy", label="语音输入")
        audio_submit = gr.Button("提交语音问题")

    with gr.Tab("文本输入"):
        text_input = gr.Textbox(label="输入文字问题")
        text_submit = gr.Button("提交文字问题")

    with gr.Column():
        recognized_text = gr.Textbox(label="识别到的提问")
        answer_text = gr.Textbox(label="回答文本")
        answer_audio = gr.Audio(label="语音回答", autoplay=True)

    # 清除按钮
    clear_btn = gr.Button("清除所有")

    # 事件处理
    audio_submit.click(
        fn=process_audio,
        inputs=audio_input,
        outputs=[recognized_text, answer_text, answer_audio]
    )

    text_submit.click(
        fn=process_text,
        inputs=text_input,
        outputs=[recognized_text, answer_text, answer_audio]
    )

    text_input.submit(
        fn=process_text,
        inputs=text_input,
        outputs=[recognized_text, answer_text, answer_audio]
    )

    clear_btn.click(
        fn=lambda: [None, "", "", None],
        outputs=[audio_input, recognized_text, answer_text, answer_audio]
    )

# 启动应用
demo.launch(debug=True)



